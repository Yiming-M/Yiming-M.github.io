<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ICME on Yiming&#39;s Portfolio</title>
    <link>http://localhost:58793/venue/icme/</link>
    <description>Recent content in ICME on Yiming&#39;s Portfolio</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 21 Dec 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:58793/venue/icme/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Interact with me: Joint Egocentric Forecasting of Intent to Interact, Attitude and Social Actions</title>
      <link>http://localhost:58793/posts/interact-with-me/</link>
      <pubDate>Sat, 21 Dec 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:58793/posts/interact-with-me/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt;: Tongfei Bian, &lt;strong&gt;Yiming Ma&lt;/strong&gt;, Mathieu Chollet, Victor Sanchez, Tanaya Guha&lt;/p&gt;&#xA;&lt;img src=&#34;posts/interact-with-me/image.png&#34; alt=&#34;&#34;&gt;&lt;p&gt;For efficient human-agent interaction, an agent should proactively recognize their target user and prepare for upcoming interactions. We formulate this challenging problem as the novel task of jointly forecasting a person&#39;s intent to interact with the agent, their attitude towards the agent and the action they will perform, from the agent&#39;s (egocentric) perspective. So we propose \emph{SocialEgoNet} - a graph-based spatiotemporal framework that exploits task dependencies through a hierarchical multitask learning approach. SocialEgoNet uses whole-body skeletons (keypoints from face, hands and body) extracted from only 1 second of video input for high inference speed. For evaluation, we augment an existing egocentric human-agent interaction dataset with new class labels and bounding box annotations. Extensive experiments on this augmented dataset, named JPL-Social, demonstrate \emph{real-time} inference and superior performance (average accuracy across all tasks: 83.15%) of our model outperforming several competitive baselines. The additional annotations and code will be available upon acceptance.&lt;/p&gt;</description>
    </item>
    <item>
      <title>CLIP-EBC: CLIP Can Count Accurately through Enhanced Blockwise Classification</title>
      <link>http://localhost:58793/posts/clip-ebc/</link>
      <pubDate>Thu, 14 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:58793/posts/clip-ebc/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt;: &lt;strong&gt;Yiming Ma&lt;/strong&gt;, Victor Sanchez, Tanaya Guha&lt;/p&gt;&#xA;&lt;p&gt;We propose CLIP-EBC, the first fully CLIP-based model for accurate crowd density estimation. While the CLIP model has demonstrated remarkable success in addressing recognition tasks such as zero-shot image classification, its potential for counting has been largely unexplored due to the inherent challenges in transforming a regression problem, such as counting, into a recognition task. In this work, we investigate and enhance CLIP&#39;s ability to count, focusing specifically on the task of estimating crowd sizes from images. Existing classification-based crowd-counting frameworks have significant limitations, including the quantization of count values into bordering real-valued bins and the sole focus on classification errors. These practices result in label ambiguity near the shared borders and inaccurate prediction of count values. Hence, directly applying CLIP within these frameworks may yield suboptimal performance.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
